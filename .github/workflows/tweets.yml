name: Scrape Tweets to Supabase (All + Priority)

on:
  schedule:
    - cron: '0 * * * *'   # hourly trigger; DST-safe gating inside
  workflow_dispatch:
    inputs:
      which_mode:
        description: 'Choose mode to run (all or priority)'
        required: false
        default: 'all'
      tweet_limit:
        description: 'Max tweets per user (optional)'
        required: false
        default: '100'

jobs:
  # 0) Cheap time gate – runs in seconds
  gate:
    runs-on: ubuntu-latest
    outputs:
      which: ${{ steps.pick.outputs.which }}   # 'all' | 'priority' | 'none'
    steps:
      - id: pick
        run: |
          HOUR=$(TZ=Europe/Paris date +%H)
          if [ "$GITHUB_EVENT_NAME" = "workflow_dispatch" ]; then
            # manual runs bypass the gate; scrape decided in next job
            echo "which=manual" >> "$GITHUB_OUTPUT"
          elif [ "$HOUR" = "04" ]; then
            echo "which=all" >> "$GITHUB_OUTPUT"
          elif [ "$HOUR" = "13" ]; then
            echo "which=priority" >> "$GITHUB_OUTPUT"
          else
            echo "which=none" >> "$GITHUB_OUTPUT"
          fi
          echo "CET/CEST hour is $HOUR → ${{ steps.pick.outputs.which }}"
  
  # 1) Scrape only when the gate says so
  scrape:
    needs: gate
    if: ${{ needs.gate.outputs.which != 'none' }}
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - uses: r-lib/actions/setup-r@v2
        with:
          r-version: '4.3'

      - name: Install system deps
        run: |
          sudo apt-get update
          sudo apt-get install -y --no-install-recommends \
            build-essential libpq-dev libssl-dev \
            libcurl4-openssl-dev libxml2-dev \
            python3-dev python3-venv libpng-dev zlib1g-dev

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Run scraper
        env:
          # Supabase
          SUPABASE_HOST:   ${{ secrets.SUPABASE_HOST }}
          SUPABASE_PORT:   ${{ secrets.SUPABASE_PORT }}
          SUPABASE_DB:     ${{ secrets.SUPABASE_DB }}
          SUPABASE_USER:   ${{ secrets.SUPABASE_USER }}
          SUPABASE_PWD:    ${{ secrets.SUPABASE_PWD }}
          # Cookies
          TW_COOKIES_JSON: ${{ secrets.TW_COOKIES_JSON }}
          # Lists (optionally repo vars)
          TW_HANDLES:           ${{ vars.TW_HANDLES }}
          TW_PRIORITY_HANDLES:  ${{ vars.TW_PRIORITY_HANDLES }}
          # Mode & limits
          SCRAPE_MODE: >-
            ${{ fromJSON('{"all":"all","priority":"priority","manual":"all"}')[needs.gate.outputs.which] }}
          TWEET_LIMIT: "100"
          PY_VENV_PATH: ".venv"
        run: Rscript fetch_twitter_to_supabase.R

  # 2) Optional job for manual runs (kept separate so gate job stays clean)
  manual:
    if: ${{ github.event_name == 'workflow_dispatch' }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: r-lib/actions/setup-r@v2
        with:
          r-version: '4.3'
      - name: Install system deps
        run: |
          sudo apt-get update
          sudo apt-get install -y --no-install-recommends \
            build-essential libpq-dev libssl-dev \
            libcurl4-openssl-dev libxml2-dev \
            python3-dev python3-venv libpng-dev zlib1g-dev
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Run scraper (manual)
        env:
          SUPABASE_HOST:   ${{ secrets.SUPABASE_HOST }}
          SUPABASE_PORT:   ${{ secrets.SUPABASE_PORT }}
          SUPABASE_DB:     ${{ secrets.SUPABASE_DB }}
          SUPABASE_USER:   ${{ secrets.SUPABASE_USER }}
          SUPABASE_PWD:    ${{ secrets.SUPABASE_PWD }}
          TW_COOKIES_JSON: ${{ secrets.TW_COOKIES_JSON }}
          TW_HANDLES:           ${{ vars.TW_HANDLES }}
          TW_PRIORITY_HANDLES:  ${{ vars.TW_PRIORITY_HANDLES }}
          SCRAPE_MODE:     ${{ inputs.which_mode }}
          TWEET_LIMIT:     ${{ inputs.tweet_limit }}
          PY_VENV_PATH:    ".venv"
        run: Rscript fetch_twitter_to_supabase.R
